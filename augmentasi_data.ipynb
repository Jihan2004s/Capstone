{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r9BZ6U5BfOGp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from CSV (assumes a column named 'text' contains the text data)\n",
        "input_file = \"machineLearning_Dataset.csv\"  # Replace with your file path\n",
        "output_file = \"augmented_dataset.csv\"\n",
        "data = pd.read_csv(input_file)"
      ],
      "metadata": {
        "id": "t-XEbsoVfUhy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the dataset has a 'text' column\n",
        "if 'description' not in data.columns:\n",
        "    raise ValueError(\"Dataset must have a 'description' column.\")\n"
      ],
      "metadata": {
        "id": "oRgcJFa_fbJY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Text Augmentation using TensorFlow\n",
        "# Tokenizer initialization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['description'])"
      ],
      "metadata": {
        "id": "X_Yh6qwQftCz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation function using random shuffling of words\n",
        "def augment_text(text):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > 1:\n",
        "        tf.random.shuffle(tokens)\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "4fZOTOM6fyM-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply augmentation\n",
        "augmented_texts = []\n",
        "for text in data['description']:\n",
        "    augmented_texts.append(augment_text(text))"
      ],
      "metadata": {
        "id": "CtTVC6iOf1aM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine original and augmented data\n",
        "augmented_data = pd.DataFrame({\n",
        "    'text': list(data['description']) + augmented_texts,\n",
        "    'label': list(data['category']) * 2  # Assuming 'label' column exists\n",
        "})\n"
      ],
      "metadata": {
        "id": "cgWQWiI_f6hp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset\n",
        "augmented_data = shuffle(augmented_data).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kiZqgR2dgE2J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a new CSV file\n",
        "augmented_data.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "ifZ3OdW-gIxF"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}